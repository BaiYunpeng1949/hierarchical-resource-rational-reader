# Parameter Inference Pipeline

This module implements a reproducible pipeline for **parameter inference** of our reading simulator.  
The workflow takes simulated results generated from a parameter grid, evaluates them against human benchmark data, and visualizes the best-fitting parameter sets.

---

## ğŸ“‚ Project Structure

parameter_inference/
â”œâ”€â”€ human_data/
â”‚ â””â”€â”€ analyzed_human_metrics.json # Ground truth metrics from human experiment
â”œâ”€â”€ simulation_data/
â”‚ â””â”€â”€ rho_0.100__w_0.500__cov_0.00/ # Example parameter combo run
â”‚ â”œâ”€â”€ all_simulation_results.json
â”‚ â”œâ”€â”€ metadata.json
â”‚ â”œâ”€â”€ processed_fixation_sequences.json (generated)
â”‚ â”œâ”€â”€ analyzed_fixation_metrics.json (generated)
â”‚ â””â”€â”€ comparison_human_vs_sim.png (generated)
â”œâ”€â”€ grid_inference_summary.csv # Summary table of all combos after inference
â”œâ”€â”€ _analyze_data.py # Functions to process & plot metrics
â”œâ”€â”€ infer_parameters.py # Runs inference, compares sims vs humans
â”œâ”€â”€ plot.py # Plots top-k parameter sets vs human
â””â”€â”€ README.md


---

## âš™ï¸ Design

1. **Simulation Data**  
   - Generated by running the reading simulator in **grid mode**.  
   - Each parameter combination (Ï, w, cov) produces a folder under `simulation_data/` with raw outputs (`all_simulation_results.json`, `metadata.json`).

2. **Human Data**  
   - `human_data/analyzed_human_metrics.json` contains aggregated behavioral metrics (reading speed, skip rate, regression rate) for baseline comparison.

3. **Analysis & Inference**  
   - `_analyze_data.py`: converts raw simulation logs into fixation sequences, per-episode metrics, and plotting utilities.  
   - `infer_parameters.py`:  
     - For each parameter-combo folder, ensures metrics are computed (calling `_analyze_data.py`).  
     - Aggregates per-condition means.  
     - Computes discrepancy (SSE or L1) vs human metrics.  
     - Produces `grid_inference_summary.csv`, sorted by loss.  

4. **Visualization**  
   - `plot.py`: reads `grid_inference_summary.csv`, selects the top-k parameter sets, and plots **human vs simulation** using the existing plotting schema.  
   - Figures and params text files are saved directly inside each best-run folder.

---

## â–¶ï¸ Reproduction Commands

### 1. Run Parameter Inference
```bash
python infer_parameters.py --grid_dir simulation_data/ --human human_data/analyzed_human_metrics.json --loss sse --norm zscore --topk 10


python plot.py \
  --grid_dir simulation_data \
  --human human_data/analyzed_human_metrics.json \
  --topk 3
```

### 2. Run the Bayesian Inference
```bash 
python bayesian_inference.py \
  --human human_data/analyzed_human_metrics.json \
  --out_root parameter_inference/bayes_runs \
  --iters 40 --init 8 --cand 512 --xi 0.01 \
  --bounds_rho 0.10 0.30 --bounds_w 0.50 1.00 --bounds_cov 0.00 3.00 \
  --stimuli 0-8 --conds 30s,60s,90s --trials 1 \
  --loss sse \
  --warm_start_from parameter_inference/simulation_data/grid_inference_summary.csv
```
So the best way would be use the grid search to roughly find some good start. Then use the Bayesian inference.

```bash
python plot.py --grid_dir parameter_inference/bayes_runs --human human_data/analyzed_human_metrics.json --topk 3
```
