import nltk
import numpy as np

from step5.modules.llm_envs.TransformerContextPredictor import TransformerContextPredictor
from step5.modules.llm_envs.TransformerLikelihoodCalculator import TransformerLikelihoodCalculator
import step5.utils.constants as const

from nltk.corpus import brown
from collections import Counter
from nltk.data import find


class BayesianSkipper:
    def __init__(self):
        """
        Initialize the Bayesian Skipper.
        Bayesian Skipper was inspired by paper:
            A Rational Model of Word Skipping in Reading: Ideal Integration of Visual and Linguistic Information
        where we frame the skip gesture as a probability-dependent decision:
            P(w|I) = normalize(P(w) * P(I|w))
        where P(w) is the prior probability of the next word w, and P(I|w) is the likelihood of the next word w given the input I.
        If P(w|I) is greater than a threshold (determined as "threshold"), we skip the next word.
        Moreover, we consider word frequency, contextual constraint, and parafovea preview's effects on the skipping decision, thus,
        P(w) = w_freq * word_freq (derived from existing dataset) + w_pred * word_contextual_constraint/predictability (generated by LLM) + Konst.
        While parafovea preview's effect (OR "Launch Site Distance" in paper A Rational Model of Word Skipping in Reading)
        is characterized by P(I|w), as shorter the word, higher the likelihood value.

        Some previous empirical findings:
            1. When not reading, but just word searching, the word frequency's effect on identification time is not significant.
            2. Word frequency's effect is much more significant compared to contextual constraint.
            3. No need to include morphological priming effect in the model because it was found not significant.
                Ref: Semantic and morphological cross-word priming during sentence reading
                Link: https://docs.google.com/presentation/d/1_GDyfsmUE4LoKYZTw1CY8x7zVsIpnSeVcFS5l_2rT-k/edit#slide=id.g272b771c0a9_0_14
            4. Word freq is dominant compared to the 5-gram predictability [Yuanyuan Duan, 2019].
        """

        print(f"{const.LV_ONE_DASHES}Bayesian Skipper -- Initialize the Bayesian Skipper.")

        # Define the tunable parameters for human data alignment
        self._skip_activation_threshold = None
        self._word_frequency_weight = None
        self._word_predictability_weight = None
        self._konst = None      # A mutable konst
        self._likelihood_weight = None  # A mutable likelihood weight

        # Define the variables
        self._skip_confidence = None    # P(w|I), if greater than this value, skip the next word, otherwise read it
        self._next_word_frequency = None    # Use existing dataset
        self._next_word_predictability = None   # Generate by LLM
        self._likelihood = None         # Handles the parafovea preview's effect

        # Initialize the LLM next word predictor, likelihood calculator, and word frequency calculator
        self.llm_predictor = TransformerContextPredictor()
        self.llm_likelihood_calculator = TransformerLikelihoodCalculator()
        self.word_frequency_calculator = WordFrequencyCalculator()

    def reset(
            self,
            skip_activation_threshold: float = 0.5,
            word_frequency_weight: float = 10,
            word_predictability_weight: float = 1,
            likelihood_weight: float = 100,
            konst: float = 1.0
    ):
        """
        Reset the Bayesian Skipper, especially the free tunable parameters.
        :return: None
        """
        self._skip_activation_threshold = skip_activation_threshold
        self._word_frequency_weight = word_frequency_weight
        self._word_predictability_weight = word_predictability_weight
        self._likelihood_weight = likelihood_weight
        self._konst = konst

    def step(self, inputs: dict = None):
        """
        Skip the next word or not.
        :return:
            skip the next word or not
        """

        # Get the next word frequency
        self._next_word_frequency = self._get_next_word_frequency(inputs=inputs)

        # Get the next word predictability
        # Two methods to determine the window length of the context information that is fed to the LLM:
        #   1. the fixed window length of 5 that is usually used in the literature, e.g., A Rational Model of Word Skipping in Reading
        #   2. the current and previous sentence with a mental representation of sentences, could use LLM to extract key gists to infer
        self._next_word_predictability = self._get_predictability(inputs=inputs)

        # Get the likelihood
        self._likelihood = self._get_likelihood(inputs=inputs)

        # Calculate the skip confidence
        self._skip_confidence = self._calculate_skip_confidence()

        # Skip the next word or not, if the skip confidence is greater than the threshold,
        #   skip the next word, otherwise read it
        return self._skip_confidence >= self._skip_activation_threshold

    def _calculate_skip_confidence(self):
        """
        Calculate the skip confidence.
        :return:
            the skip confidence
        """
        confidence = (self._word_frequency_weight * self._next_word_frequency + self._word_predictability_weight * self._next_word_predictability + self._konst) * (self._likelihood_weight * self._likelihood)
        return np.clip(confidence, 0, 1)

    def _get_next_word_frequency(self, inputs: dict = None):
        """
        Get the frequency of the next word.
        :return:
            the frequency of the next word
        """
        return self.word_frequency_calculator.get_probability(word=inputs['next_word'])

    def _get_predictability(self, inputs: dict = None):
        """
        Get the predictability of the next word.
        :return:
            the predictability of the next word
        """
        return self.llm_predictor.predict(context_words=inputs['context_words'], next_word=inputs['next_word'])

    def _get_likelihood(self, inputs: dict = None):
        """
        Get the likelihood of the next word.
        :return:
            the likelihood of the next word
        """
        return self.llm_likelihood_calculator.get_likelihood(
            parafovea_letters=inputs['parafovea_letters'],
            next_word=inputs['next_word'],
            estimated_next_word_len=inputs['estimated_word_len']
        )

    def log(self):
        """
        Log the Bayesian Skipper's parameters.
        :return: a dictionary of the Bayesian Skipper's parameters and values
        """
        return {
            'Skip Decision': self._skip_confidence >= self._skip_activation_threshold,
            'Skip Confidence': self._skip_confidence,
            'Skip Activation Threshold': self._skip_activation_threshold,
            'Word Frequency Weight': self._word_frequency_weight,
            'Word Predictability Weight': self._word_predictability_weight,
            'Konst': self._konst,
            'Next Word Frequency': self._next_word_frequency,
            'Next Word Predictability': self._next_word_predictability,
            'Likelihood': self._likelihood,
        }


class WordFrequencyCalculator:

    def __init__(self):
        """
        Initialize the Word Frequency.
        """
        # Check if the Brown corpus is available
        try:
            find('corpora/brown')
            # print(f"{cons.LV_TWO_DASHES}Word Frequency Calculator.\n"
            #       f"{cons.LV_THREE_DASHES}Get the Word Frequency from the Brown corpus. \n"
            #       f"{cons.LV_FOUR_DASHES}The Brown corpus is available.")
        except LookupError:
            print(f"Downloading the Brown corpus...")
            nltk.download('brown', quiet=True)

        # Load the Brown corpus
        words = brown.words()

        # Calculate word frequencies
        word_counts = Counter(words)
        total_words = sum(word_counts.values())

        # Normalize frequencies to probabilities
        self.word_probabilities = {word: count / total_words for word, count in word_counts.items()}

    def get_probability(self, word: str) -> float:
        """
        Get the probability of a word.
        :param word: the word
        :return: the probability of the word
        """
        return self.word_probabilities.get(word, 0.0)


if __name__ == "__main__":

    # Test my API here
    bs = BayesianSkipper()
    bs.reset(
        skip_activation_threshold=0.5,
        word_frequency_weight=10,
        word_predictability_weight=0.5,
        konst=0.1
    )
    _inputs = {
        'context_words': 'the leaves of',
        'next_word': 'the',
        'parafovea_letters': 'the',
        'estimated_word_len': 3,
    }
    skip_decision = bs.step(inputs=_inputs)
    print(f"Skip the next word: {skip_decision}")
    current_log = bs.log()
    print(f"Current log: {current_log}")
