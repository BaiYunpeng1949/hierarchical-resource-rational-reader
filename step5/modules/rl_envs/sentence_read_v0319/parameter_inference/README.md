# Sentence Reading — Grid Search for `w_regression_cost`

This README explains how to reproduce the sentence‑reading **grid search** and the resulting **figures** for the parameter **`w_regression_cost`** using the current plot‑matching scorer.

The scorer compares **linear regression lines** (slope + intercept, via `numpy.polyfit`) between human and simulation curves on the *overlap* of x‑ranges, exactly as in `plot.py`:

- Skip vs **length**
- Skip vs **logit predictability**
- Skip vs **log frequency**
- **Regression** vs **difficulty**

The objective for a folder is:
\[
F = \sum_{\text{curves}} \Big[ (s_{\text{sim}} - s_{\text{hum}})^2 + (b_{\text{sim}} - b_{\text{hum}})^2 \Big]
\]
where \(s\) is slope and \(b\) is intercept from `np.polyfit(x, y, deg=1)` on the **overlap** of the human and sim x‑ranges.

---

## 1) Expected layout

```
parameter_inference/
  grid_search_w_regression_cost.py
  plot.py
  human_data/
    all_words_regression_and_skip_probabilities.csv
  simulation_data/
    w_regression_cost_0p00/
      all_words_regression_and_skip_probabilities.csv
    w_regression_cost_0p02/
      all_words_regression_and_skip_probabilities.csv
    ...
  figures/                # created by the grid search when plotting
```

- Each `w_regression_cost_*` folder should contain the **analyzed** CSV
  `all_words_regression_and_skip_probabilities.csv` (produced from the raw logs).
- The human CSV must include columns:
  `length, logit_predictability, log_frequency, difficulty,
   skip_probability, regression_probability`.

---

## 2) Configure the grid (in `config.yaml`)

Set the sentence‑reading run to **grid test** and choose the search range:

```yaml
# Example — adapt to your project keys
mode: sentence_grid_test

sentence_grid_test:
  w_regression_cost:
    start: 0.00
    end:   1.00
    step:  0.02
  episodes: 10        # episodes per setting (example)
  save_raw: true      # save raw logs per setting
  analyze:  true      # run analysis to produce the per-folder CSVs
```

> The grid will create `simulation_data/w_regression_cost_*` folders and, if `analyze: true`, write `all_words_regression_and_skip_probabilities.csv` into each.

Start the grid run from your repo root (whichever entry point your project uses):

```bash
# If main.py triggers STB3RL._sentence_reading_grid_test()
python main.py

# OR if you have a dedicated script
python sentence_read_grid_test.py
```

---

## 3) Run the grid scorer + generate figures

From `parameter_inference/`:

```bash
python grid_search_w_regression_cost.py   --human human_data/all_words_regression_and_skip_probabilities.csv   --sim_root simulation_data
```

### Outputs
- **Ranking CSV:** `simulation_data/grid_search_w_regression_cost_results.csv`
- **Console:** prints best `w_regression_cost`, per‑curve losses, and `F_total`
- **Figures:** `parameter_inference/figures/`
  - `skip_vs_length_binned_only_regression.png`
  - `skip_vs_logit_predictability_binned_only_regression.png`
  - `skip_vs_log_frequency_binned_only_regression.png`
  - `regression_vs_difficulty_binned_only_regression.png`
- **Best record:** `parameter_inference/figures/best_param.txt`

> Figures are generated by importing your `plot.py` and passing the human vs best‑sim data frames. No plotting happens during simulation—only after the best setting is found.

---

## 4) Notes on the loss / weighting

By default, slope and intercept differences are equally weighted. To emphasize **trend** alignment, open `grid_search_w_regression_cost.py` and adjust the small constants near the top:

```python
# loss config
NORMALIZE = True        # normalize x,y by human ranges (balances scales across curves)
SLOPE_WEIGHT = 10.0     # prioritize slopes
INTERCEPT_WEIGHT = 1.0

CURVE_WEIGHTS = {       # optional per-curve weights
    "skip_vs_length": 1.0,
    "skip_vs_logit_predictability": 1.0,
    "skip_vs_log_frequency": 1.0,
    "regression_vs_difficulty": 1.0,
}
```

- `NORMALIZE=True` makes slopes/offsets comparable across curves.
- Increase `SLOPE_WEIGHT` (e.g., 10.0) if trend matching is more important.
- If one curve dominates, down‑weight it with `CURVE_WEIGHTS` (e.g., set `"regression_vs_difficulty": 0.75`).

---

## 5) Troubleshooting

- **Figures step fails with “No such file or directory”**: ensure `--sim_root` points to the directory **containing** the `w_regression_cost_*` folders. The script reads sim CSV from `<sim_root>/<best_folder>/<sim_relpath>`.
- **A curve prints `NA`**: verify those columns exist in both CSVs and that there are **≥ 2** points per side in the overlap of x‑ranges.
- **Folder naming**: folders should follow `w_regression_cost_XpYY` (or `w_regression_cost_<float>`). This only affects pretty‑printed values; data loading uses the paths directly.
